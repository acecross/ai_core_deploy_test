apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: abap-rag-server
  annotations:
    scenarios.ai.sap.com/name: abap-rag
    executables.ai.sap.com/name: rag-fastapi
spec:
  inputs:
    image:
      type: string
    port:
      type: string
      default: "8080"

  # IMPORTANT: the KServe spec must be a single multiline string
  template: |
    apiVersion: serving.kserve.io/v1beta1
    kind: InferenceService
    metadata:
      name: "{{ name }}"
      labels:
        ai.sap.com/resourcePlan: infer.s
    spec:
      predictor:
        imagePullSecrets:
          - name: ghcr         # remove this block if your image is public
        containers:
          - name: api
            image: "{{ image }}"
            ports:
              - containerPort: {{ port }}
            env:
              - name: PORT
                value: "{{ port }}"
              - name: LLM_PROVIDER
                value: "aicore"
              - name: LLM_MODEL
                value: "gpt-4o-mini"
            readinessProbe:
              httpGet:
                path: /healthz
                port: {{ port }}
              initialDelaySeconds: 5
              periodSeconds: 5
            livenessProbe:
              httpGet:
                path: /healthz
                port: {{ port }}
              initialDelaySeconds: 15
              periodSeconds: 10

  resources:
    minReplicas: 1
    maxReplicas: 2
